{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install openai langchain langchain-openai huggingface_hub replicate tiktoken datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = ''\n",
    "# os.environ['HUGGINGFACEHUG_API_TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_openai = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.9, max_tokens=256)\n",
    "# llm_hf = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", temperature= 0.5, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_history_aware_retriever_1(llm, rag_retriever):\n",
    "    ### Contextualize question ###\n",
    "    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "    which might reference context in the chat history, formulate a standalone question \\\n",
    "    which can be understood without the chat history. Do NOT answer the question, \\\n",
    "    just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, rag_retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "    return history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_answer_chain(llm):\n",
    "    ### Answer question ###\n",
    "    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "    {context}\"\"\"\n",
    "    \n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    return question_answer_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_chain(llm, rag_retriever):\n",
    "    history_aware_retriever = create_history_aware_retriever_1(llm, rag_retriever)\n",
    "    question_answer_chain = create_question_answer_chain(llm)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    \n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "def create_conversational_rag_chain(rag_chain):\n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\",\n",
    "    )\n",
    "    return conversational_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "\n",
    "def get_pgvector_store():\n",
    "    CONNECTION_STRING = \"postgresql+psycopg2://appuser:devpassword@localhost:5432/devdb\"\n",
    "    COLLECTION_NAME = \"qmap_backend_repo\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    db = PGVector(connection_string=CONNECTION_STRING, embedding_length=1536, \n",
    "                collection_name=COLLECTION_NAME, embedding_function=embeddings,\n",
    "                use_jsonb=True)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = get_pgvector_store()\n",
    "llm = llm_openai\n",
    "rag_chain = create_rag_chain(llm, vector_store.as_retriever())\n",
    "conversational_rag_chain = create_conversational_rag_chain(rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_qa(chat_id, input):\n",
    "    response = conversational_rag_chain.invoke({\"input\": input}, config={\"configurable\": {\"session_id\": chat_id}})\n",
    "\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The members of the TechPhantoms team at Qapita are Arulmani (Backend engineer), Dinesh (Frontend engineer), Mrityunjoy (Backend engineer), Gouri (Product manager), Siddharth (Frontend engineer), Kamlesh (QA Engineer), and Ikram (Frontend engineer). They work together on Qapita's flagship Equity Management Platform, QapMap, utilizing various technologies for both backend and frontend development."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(ask_qa(\"vmk_1\", \"Who are the members of TechPhantoms team?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There is one QA Engineer, Kamlesh, in the TechPhantoms team at Qapita. Kamlesh plays a vital role in ensuring the quality and reliability of Qapita's Equity Management Platform, QapMap."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(ask_qa(\"vmk_1\", \"How many QA engineers are there in the team?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided information, the TechPhantoms team at Qapita currently has two backend engineers, Arulmani and Mrityunjoy. Whether the team needs more backend engineers would depend on their current workload, projects in the pipeline, and growth plans. Additional context would be required to determine if more backend engineers are needed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(ask_qa(\"vmk_1\", \"Do we need more backend engineers in the team?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain-dev)",
   "language": "python",
   "name": "langchain-dev-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
