{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Pip install necessary package\n",
    "%pip install --upgrade --quiet  pgvector\n",
    "%pip install --upgrade --quiet  langchain-openai\n",
    "%pip install --upgrade --quiet  psycopg2-binary\n",
    "%pip install --upgrade --quiet  tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "from langchain.indexes import SQLRecordManager, index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_STRING = \"postgresql+psycopg2://appuser:devpassword@localhost:5433/devdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"qmap_backend_repo\"\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "db = PGVector(connection_string=CONNECTION_STRING, embedding_length=1536, \n",
    "              collection_name=COLLECTION_NAME, embedding_function=embeddings,\n",
    "              use_jsonb=True)\n",
    "\n",
    "\n",
    "# https://python.langchain.com/docs/modules/data_connection/indexing/\n",
    "namespace = f\"postgres/{COLLECTION_NAME}\"\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace, db_url=CONNECTION_STRING\n",
    ")\n",
    "\n",
    "record_manager.create_schema()\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    documents=[],\n",
    "    use_jsonb=True,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_docs(dir: str):\n",
    "    loader = DirectoryLoader(dir, glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    for _, doc in enumerate(docs):\n",
    "        metadata = doc.metadata\n",
    "        \n",
    "    return docs\n",
    "\n",
    "def print_docs(docs):\n",
    "    doc: Document\n",
    "    for _, doc in enumerate(docs):\n",
    "        print(doc.page_content)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = load_docs(\"./rag-data\")\n",
    "\n",
    "index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    db,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "# db.add_documents(docs)\n",
    "# print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['PGVector', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.pgvector.PGVector object at 0x74676860bfe0>\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "# Can be any valid LLM\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", openai_api_key=\"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\")\n",
    "# llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Relevant pieces of previous conversation:\n",
    "{history}\n",
    "\n",
    "(You do not need to use these pieces of information if not relevant)\n",
    "\n",
    "Current conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=_DEFAULT_TEMPLATE\n",
    ")\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Relevant pieces of previous conversation:\n",
      "Hiiiiiiiiii\n",
      "\n",
      "Migration team consists of abhinandhu and arjun\n",
      "input: Describe the architecture of QapMap?\n",
      "response: QapMap is a distributed system architecture that consists of multiple layers. At the core, there are data storage nodes that store the map data and metadata. These nodes are connected to a set of query processing nodes that handle user queries and perform computations on the data. Additionally, there are communication nodes that facilitate communication between the data storage and query processing nodes. The architecture is designed to be scalable and fault-tolerant, with redundancy built in to ensure high availability. Overall, QapMap's architecture is optimized for efficient data retrieval and processing in a distributed environment.\n",
      "input: Describe the architecture of QapMap?\n",
      "response: QapMap is a distributed system architecture that consists of multiple layers. At the core, there are data storage nodes that store the map data and metadata. These nodes are connected to a set of query processing nodes that handle user queries and perform computations on the data. Additionally, there are communication nodes that facilitate communication between the data storage and query processing nodes. The architecture is designed to be scalable and fault-tolerant, with redundancy built in to ensure high availability. Overall, QapMap's architecture is optimized for efficient data retrieval and processing in a distributed environment.\n",
      "\n",
      "(You do not need to use these pieces of information if not relevant)\n",
      "\n",
      "Current conversation:\n",
      "Human: Describe the migration team?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The migration team consists of Abhinandhu and Arjun. They are responsible for migrating data and services from one system to another, ensuring a smooth transition and minimal disruption to users. Abhinandhu and Arjun work closely with the development and operations teams to plan and execute the migration process, taking into account factors such as data integrity, security, and performance. Their expertise in data migration tools and techniques helps ensure that the migration is completed successfully and on schedule.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation_with_summary.predict(input=\"Who are the founders of Qapita?\")\n",
    "conversation_with_summary.predict(input=\"Describe the migration team?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zsh-Hw5KYUKy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
